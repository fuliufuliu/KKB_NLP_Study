{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://www.cnblogs.com/wwj99/p/12503545.html\n",
    "BPE 算法\n",
    "\n",
    "GPT-2 模型在数据预处理时使用了字节对编码（Byte Pair Encoding，简称 BPE）方法，BPE 是一种能够解决未登录词问题，并减小词典大小的方法。它综合利用了单词层面编码和字符层面编码的优势，举例来说，我们要对下面的字符串编码，\n",
    "\n",
    "aaabdaaabac\n",
    "字节对 aa 出现的次数最多，所以我们将它替换成一个没在字符串中被用过的字符 Z ，\n",
    "\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "然后我们重复这个过程，用 Y 替换 ab ，\n",
    "\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "继续，用 X 替换 ZY ，\n",
    "\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq  # 计算字节对出现频率\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))  # 将字节对中可解释为正则运算符的字符转义\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # 将要合并的字节对前后只能为空白字符\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)  # 合并符合条件的字节对\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,\n",
    "         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 选择频率最大的字节对\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)\n",
    "print(vocab)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# top-k\n",
    "import random\n",
    "\n",
    "def select_top_k(predictions, k=10):\n",
    "    predicted_index = random.choice(\n",
    "        predictions[0, -1, :].sort(descending=True)[1][:10]).item()\n",
    "    return predicted_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 使用在 PyTorch-Transformers 模型库中封装好的 GPT2Tokenizer() 和 GPT2LMHeadModel()\n",
    "# 安装 PyTorch-Transformers\n",
    "# !pip install pytorch_transformers==1.0\n",
    "# !pip install pytorch_transformers==1.0 -i  https://pypi.tuna.tsinghua.edu.cn/simple/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache, downloading to C:\\Users\\fuliu\\AppData\\Local\\Temp\\tmp5sa6c9a8\n",
      "\n",
      "  0%|          | 0/456318 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|          | 1024/456318 [00:01<08:47, 862.80B/s]\u001b[A\n",
      "  4%|▎         | 16384/456318 [00:02<06:05, 1204.83B/s]\u001b[A\n",
      "  4%|▍         | 17408/456318 [00:09<19:25, 376.58B/s] \u001b[A\n",
      "  7%|▋         | 33792/456318 [00:09<13:07, 536.71B/s]\u001b[A\n",
      "  8%|▊         | 36864/456318 [00:17<14:37, 477.90B/s]\u001b[A\n",
      " 11%|█         | 51200/456318 [00:18<09:57, 678.11B/s]\u001b[A\n",
      " 12%|█▏        | 53248/456318 [00:21<10:48, 621.28B/s]\u001b[A\n",
      " 15%|█▌        | 69632/456318 [00:24<07:32, 853.91B/s]\u001b[A\n",
      " 19%|█▉        | 87040/456318 [00:30<05:40, 1083.14B/s]\u001b[A\n",
      " 23%|██▎       | 103424/456318 [00:30<03:49, 1537.01B/s]\u001b[A\n",
      " 23%|██▎       | 106496/456318 [00:35<05:37, 1035.18B/s]\u001b[A\n",
      " 27%|██▋       | 121856/456318 [00:41<04:23, 1269.50B/s]\u001b[A\n",
      " 31%|███       | 139264/456318 [00:43<03:05, 1709.20B/s]\u001b[A\n",
      " 34%|███▍      | 156672/456318 [00:47<02:21, 2118.15B/s]\u001b[A\n",
      " 38%|███▊      | 174080/456318 [00:50<01:47, 2629.60B/s]\u001b[A\n",
      " 42%|████▏     | 191488/456318 [00:52<01:21, 3252.22B/s]\u001b[A\n",
      " 46%|████▌     | 208896/456318 [00:53<00:59, 4169.32B/s]\u001b[A\n",
      " 50%|████▉     | 226304/456318 [00:56<00:49, 4668.11B/s]\u001b[A\n",
      " 53%|█████▎    | 243712/456318 [01:03<00:59, 3601.50B/s]\u001b[A\n",
      " 57%|█████▋    | 260096/456318 [01:04<00:40, 4824.88B/s]\u001b[A\n",
      " 57%|█████▋    | 262144/456318 [01:09<02:42, 1192.51B/s]\u001b[A\n",
      " 61%|██████    | 278528/456318 [01:21<02:24, 1230.66B/s]\u001b[A\n",
      " 65%|██████▍   | 294912/456318 [01:22<01:33, 1718.62B/s]\u001b[A\n",
      " 65%|██████▍   | 294912/456318 [01:34<01:33, 1718.62B/s]\u001b[A\n",
      " 65%|██████▍   | 295936/456318 [01:39<14:45, 181.16B/s] \u001b[A\n",
      " 68%|██████▊   | 312320/456318 [01:40<09:17, 258.22B/s]\u001b[A\n",
      " 68%|██████▊   | 312320/456318 [01:54<09:17, 258.22B/s]\u001b[A\n",
      " 69%|██████▊   | 313344/456318 [03:09<1:08:21, 34.86B/s]\u001b[A\n",
      " 72%|███████▏  | 329728/456318 [03:16<42:38, 49.47B/s]  \u001b[A\n",
      " 72%|███████▏  | 329728/456318 [03:34<42:38, 49.47B/s]\u001b[A\n",
      " 72%|███████▏  | 330752/456318 [06:32<2:30:10, 13.94B/s]\u001b[A\n",
      " 76%|███████▌  | 347136/456318 [06:38<1:31:35, 19.87B/s]\u001b[A\n",
      " 76%|███████▌  | 347136/456318 [06:54<1:31:35, 19.87B/s]\u001b[A\n",
      " 76%|███████▋  | 348160/456318 [08:03<1:48:21, 16.64B/s]\u001b[A\n",
      " 80%|███████▉  | 364544/456318 [08:07<1:04:28, 23.72B/s]\u001b[A\n",
      " 80%|███████▉  | 364544/456318 [08:24<1:04:28, 23.72B/s]\u001b[A\n",
      " 80%|████████  | 365568/456318 [09:45<1:28:16, 17.13B/s]\u001b[A\n",
      " 84%|████████▎ | 381952/456318 [09:48<50:42, 24.44B/s]  \u001b[A\n",
      " 84%|████████▎ | 381952/456318 [10:04<50:42, 24.44B/s]\u001b[A\n",
      " 84%|████████▍ | 382976/456318 [11:00<1:00:45, 20.12B/s]\u001b[A\n",
      " 88%|████████▊ | 399360/456318 [11:03<33:05, 28.69B/s]  \u001b[A\n",
      " 88%|████████▊ | 399360/456318 [11:14<33:05, 28.69B/s]\u001b[A\n",
      " 88%|████████▊ | 400384/456318 [12:03<38:57, 23.93B/s]\u001b[A\n",
      " 91%|█████████▏| 416768/456318 [12:06<19:19, 34.11B/s]\u001b[A\n",
      " 91%|█████████▏| 416768/456318 [12:24<19:19, 34.11B/s]\u001b[A\n",
      " 92%|█████████▏| 417792/456318 [13:39<30:42, 20.91B/s]\u001b[A\n",
      " 95%|█████████▌| 434176/456318 [13:43<12:22, 29.81B/s]\u001b[A\n",
      " 95%|█████████▌| 434176/456318 [13:54<12:22, 29.81B/s]\u001b[A\n",
      " 95%|█████████▌| 435200/456318 [15:04<16:35, 21.21B/s]\u001b[A\n",
      " 99%|█████████▉| 451584/456318 [15:07<02:36, 30.25B/s]\u001b[A\n",
      " 99%|█████████▉| 451584/456318 [15:24<02:36, 30.25B/s]\u001b[A\n",
      "100%|██████████| 456318/456318 [15:27<00:00, 491.80B/s][A\n",
      "INFO:pytorch_transformers.file_utils:copying C:\\Users\\fuliu\\AppData\\Local\\Temp\\tmp5sa6c9a8 to cache at C:\\Users\\fuliu\\.cache\\torch\\pytorch_transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for C:\\Users\\fuliu\\.cache\\torch\\pytorch_transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:pytorch_transformers.file_utils:removing temp file C:\\Users\\fuliu\\AppData\\Local\\Temp\\tmp5sa6c9a8\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\fuliu\\.cache\\torch\\pytorch_transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\fuliu\\.cache\\torch\\pytorch_transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 12])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertModel, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 载入预训练模型的分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 使用 GPT2Tokenizer 对输入进行编码\n",
    "text = \"Yesterday, a man named Jack said he saw an alien,\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(indexed_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorch_transformers import GPT2LMHeadModel\n",
    "\n",
    "# 读取 GPT-2 预训练模型\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "total_predicted_text = text\n",
    "n = 100  # 预测过程的循环次数\n",
    "for _ in range(n):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    predicted_index = select_top_k(predictions, k=10)\n",
    "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "    total_predicted_text += tokenizer.decode(predicted_index)\n",
    "\n",
    "    if '<|endoftext|>' in total_predicted_text:\n",
    "        # 如果出现文本结束标志，就结束文本生成\n",
    "        break\n",
    "\n",
    "    indexed_tokens += [predicted_index]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "print(total_predicted_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "微调生成戏剧文本\n",
    "接下来，我们将使用一些戏剧剧本对 GPT-2 进行微调。由于 OpenAI 团队开源的 GPT-2 模型预训练参数为使用英文数据集预训练后得到的，虽然可以在微调时使用中文数据集，但需要大量数据和时间才会有好的效果，所以这里我们使用了英文数据集进行微调，从而更好地展现 GPT-2 模型的能力。\n",
    "\n",
    "首先，下载训练数据集，这里使用了莎士比亚的戏剧作品《罗密欧与朱丽叶》作为训练样本。数据集已经提前下载好并放在云盘中，链接：https://pan.baidu.com/s/1LiTgiake1KC8qptjRncJ5w 提取码：km06"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('./romeo_and_juliet.txt', 'r') as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 预处理训练集，将训练集编码、分段。\n",
    "indexed_text = tokenizer.encode(dataset)\n",
    "del(dataset)\n",
    "\n",
    "dataset_cut = []\n",
    "for i in range(len(indexed_text)//512):\n",
    "    # 将字符串分段成长度为 512\n",
    "    dataset_cut.append(indexed_text[i*512:i*512+512])\n",
    "del(indexed_text)\n",
    "\n",
    "dataset_tensor = torch.tensor(dataset_cut)\n",
    "dataset_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 这里使用 PyTorch 提供的 DataLoader() 构建训练集数据集表示，使用 TensorDataset() 构建训练集数据迭代器。\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 构建数据集和数据迭代器，设定 batch_size 大小为 2\n",
    "train_set = TensorDataset(dataset_tensor,\n",
    "                          dataset_tensor)  # 标签与样本数据相同\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=2,\n",
    "                          shuffle=False)\n",
    "train_loader\n",
    "\n",
    "\n",
    "# 检查是否机器有 GPU，如果有就在 GPU 运行，否则就在 CPU 运行。\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info('device: ', device)\n",
    "\n",
    "# 开始训练\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "pre = time.time()\n",
    "\n",
    "epoch = 30  # 循环学习 30 次\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 定义优化器\n",
    "\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, _ = model(data, labels=target)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx == len(train_loader)-1:\n",
    "            # 在每个 Epoch 的最后输出一下结果\n",
    "            print('average loss:', total_loss/len(train_loader))\n",
    "\n",
    "print('训练时间：', time.time()-pre)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练结束后，可以使模型生成文本，观察输出。\n",
    "text = \"From fairest creatures we desire\"  # 这里也可以输入不同的英文文本\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "model.eval()\n",
    "total_predicted_text = text\n",
    "\n",
    "# 使训练后的模型进行 500 次预测\n",
    "for _ in range(500):\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    predicted_index = select_top_k(predictions, k=10)\n",
    "\n",
    "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "    total_predicted_text += tokenizer.decode(predicted_index)\n",
    "    if '<|endoftext|>' in total_predicted_text:\n",
    "        # 如果出现文本结束标志，就结束文本生成\n",
    "        break\n",
    "\n",
    "    indexed_tokens += [predicted_index]\n",
    "\n",
    "    if len(indexed_tokens) > 1023:\n",
    "        # 模型最长输入长度为1024，如果长度过长则截断\n",
    "        indexed_tokens = indexed_tokens[-1023:]\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "print(total_predicted_text)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
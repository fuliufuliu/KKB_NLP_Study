{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"GPT_2_4_Story.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"pycharm":{"name":"#%% md\n"},"id":"662f40saZXgy","colab_type":"text"},"source":["https://www.cnblogs.com/wwj99/p/12503545.html\n","BPE 算法\n","\n","GPT-2 模型在数据预处理时使用了字节对编码（Byte Pair Encoding，简称 BPE）方法，BPE 是一种能够解决未登录词问题，并减小词典大小的方法。它综合利用了单词层面编码和字符层面编码的优势，举例来说，我们要对下面的字符串编码，\n","\n","aaabdaaabac\n","字节对 aa 出现的次数最多，所以我们将它替换成一个没在字符串中被用过的字符 Z ，\n","\n","ZabdZabac\n","Z=aa\n","然后我们重复这个过程，用 Y 替换 ab ，\n","\n","ZYdZYac\n","Y=ab\n","Z=aa\n","继续，用 X 替换 ZY ，\n","\n","XdXac\n","X=ZY\n","Y=ab\n","Z=aa"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"NGVCpgE1ZXgz","colab_type":"code","outputId":"60da651f-598a-438a-dcbc-730d4da7cd6c","executionInfo":{"status":"ok","timestamp":1586533635415,"user_tz":-480,"elapsed":2534,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["import re\n","import collections\n","\n","def get_stats(vocab):\n","    pairs = collections.defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols)-1):\n","            pairs[symbols[i], symbols[i+1]] += freq  # 计算字节对出现频率\n","    return pairs\n","\n","\n","def merge_vocab(pair, v_in):\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))  # 将字节对中可解释为正则运算符的字符转义\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # 将要合并的字节对前后只能为空白字符\n","    for word in v_in:\n","        w_out = p.sub(''.join(pair), word)  # 合并符合条件的字节对\n","        v_out[w_out] = v_in[word]\n","    return v_out\n","\n","vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,\n","         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n","num_merges = 10\n","for i in range(num_merges):\n","    pairs = get_stats(vocab)\n","    best = max(pairs, key=pairs.get)  # 选择频率最大的字节对\n","    vocab = merge_vocab(best, vocab)\n","    print(best)\n","print(vocab)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('e', 's')\n","('es', 't')\n","('est', '</w>')\n","('l', 'o')\n","('lo', 'w')\n","('n', 'e')\n","('ne', 'w')\n","('new', 'est</w>')\n","('low', '</w>')\n","('w', 'i')\n","{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"tiY7_pDBZXg4","colab_type":"code","colab":{}},"source":["# top-k\n","import random\n","\n","def select_top_k(predictions, k=10):\n","    predicted_index = random.choice(\n","        predictions[0, -1, :].sort(descending=True)[1][:10]).item()\n","    return predicted_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"sd2xKkwJZXg7","colab_type":"code","outputId":"1dabcb22-9153-4325-f332-713d6303b7bd","executionInfo":{"status":"ok","timestamp":1586533643028,"user_tz":-480,"elapsed":10125,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":449}},"source":["# 使用在 PyTorch-Transformers 模型库中封装好的 GPT2Tokenizer() 和 GPT2LMHeadModel()\n","# 安装 PyTorch-Transformers\n","!pip install pytorch_transformers==1.0\n","# !pip install pytorch_transformers==1.0 -i  https://pypi.tuna.tsinghua.edu.cn/simple/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pytorch_transformers==1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n","\r\u001b[K     |██▍                             | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.4.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\r\u001b[K     |▎                               | 10kB 32.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 37.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 44.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 46.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 47.2MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 50.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 52.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 50.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 52.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 54.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 54.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 54.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 54.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 54.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 54.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.12.35)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.18.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (4.38.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (2.8)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (1.15.35)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch_transformers==1.0) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch_transformers==1.0) (0.15.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.35->boto3->pytorch_transformers==1.0) (1.12.0)\n","Installing collected packages: sentencepiece, pytorch-transformers\n","Successfully installed pytorch-transformers-1.0.0 sentencepiece-0.1.85\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W7UHdFnymh9o","colab_type":"code","colab":{}},"source":["# https://huggingface.co/transformers/pretrained_models.html?highlight=bert%20base\n","# bert-base-chinese"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"L56M6lHfZXg9","colab_type":"code","outputId":"b2c6cfc0-3552-4090-bc35-d433f7fc1954","executionInfo":{"status":"ok","timestamp":1586533646208,"user_tz":-480,"elapsed":13295,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":271}},"source":["import torch\n","from pytorch_transformers import BertModel, GPT2Model, GPT2Tokenizer\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# 载入预训练模型的分词器\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# 使用 GPT2Tokenizer 对输入进行编码\n","# text = \"Yesterday, a man named Jack said he saw an alien,\"\n","text = \"昨天, 一个名叫杰克的人说他看到了一头狮子,\"\n","indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","tokens_tensor.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache, downloading to /tmp/tmpe3ttrjl6\n","100%|██████████| 1042301/1042301 [00:00<00:00, 2774156.88B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmpe3ttrjl6 to cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpe3ttrjl6\n","INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache, downloading to /tmp/tmppvrrgn42\n","100%|██████████| 456318/456318 [00:00<00:00, 1764624.48B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmppvrrgn42 to cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmppvrrgn42\n","INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 35])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"iogM1riQZXhA","colab_type":"code","outputId":"b1956739-abfe-4d1d-abed-29a953b9d712","executionInfo":{"status":"ok","timestamp":1586533646209,"user_tz":-480,"elapsed":13283,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(indexed_tokens)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[23626, 101, 25465, 11, 220, 31660, 10310, 103, 28938, 235, 20998, 104, 30266, 108, 17739, 233, 21410, 21689, 46237, 112, 20015, 244, 40367, 233, 26344, 108, 12859, 228, 31660, 13783, 112, 45379, 106, 36310, 11]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"cbK2k_QcZXhD","colab_type":"code","outputId":"7a3506b1-6534-48da-91fd-d3ee59edf566","executionInfo":{"status":"ok","timestamp":1586533733430,"user_tz":-480,"elapsed":100492,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":757}},"source":["from pytorch_transformers import GPT2LMHeadModel\n","\n","# 读取 GPT-2 预训练模型\n","# model = GPT2LMHeadModel.from_pretrained(\"./\")\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.eval()\n","\n","total_predicted_text = text\n","n = 100  # 预测过程的循环次数\n","for _ in range(n):\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache, downloading to /tmp/tmppn7o5v9f\n","100%|██████████| 224/224 [00:00<00:00, 56312.88B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmppn7o5v9f to cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmppn7o5v9f\n","INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n","INFO:pytorch_transformers.modeling_utils:Model config {\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"embd_pdrop\": 0.1,\n","  \"finetuning_task\": null,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"num_labels\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"token_ids\",\n","  \"summary_use_proj\": true,\n","  \"torchscript\": false,\n","  \"vocab_size\": 50257\n","}\n","\n","INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache, downloading to /tmp/tmp2pv1_f54\n","100%|██████████| 548118077/548118077 [00:35<00:00, 15474556.87B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmp2pv1_f54 to cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp2pv1_f54\n","INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"],"name":"stderr"},{"output_type":"stream","text":["昨天, 一个名叫杰克的人说他看到了一头狮子,������的��。����不�����的�������������,���������������,不���������的�������,������的������������的人�������,��\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"nkakEo03ZXhG","colab_type":"text"},"source":["微调生成戏剧文本\n","接下来，我们将使用一些戏剧剧本对 GPT-2 进行微调。由于 OpenAI 团队开源的 GPT-2 模型预训练参数为使用英文数据集预训练后得到的，虽然可以在微调时使用中文数据集，但需要大量数据和时间才会有好的效果，所以这里我们使用了英文数据集进行微调，从而更好地展现 GPT-2 模型的能力。\n","\n","首先，下载训练数据集，这里使用了莎士比亚的戏剧作品《罗密欧与朱丽叶》作为训练样本。数据集已经提前下载好并放在云盘中，链接：https://pan.baidu.com/s/1LiTgiake1KC8qptjRncJ5w 提取码：km06"]},{"cell_type":"code","metadata":{"id":"fS8RpP9UaMX2","colab_type":"code","outputId":"ca817b8e-ae25-45fc-e4b5-4d4d2af4d498","executionInfo":{"status":"ok","timestamp":1586535096411,"user_tz":-480,"elapsed":1463462,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/Colab Notebooks/Lesson12"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/Lesson12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"9tpWJ3D_ZXhK","colab_type":"code","outputId":"fee7a993-4100-4596-a53b-a138ed73f4c6","executionInfo":{"status":"ok","timestamp":1586546734896,"user_tz":-480,"elapsed":7881010,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# with open('./Datas/romeo_and_juliet.txt', 'r') as f:星辰变\n","with open('./Datas/星辰变.txt', 'r') as f:\n","    dataset = f.read()\n","\n","print(len(dataset))\n","\n","dataset = dataset[:len(dataset)//50]\n","\n","# 预处理训练集，将训练集编码、分段。\n","indexed_text = tokenizer.encode(dataset)\n","del(dataset)\n","\n","dataset_cut = []\n","for i in range(len(indexed_text)//512):\n","    # 将字符串分段成长度为 512\n","    dataset_cut.append(indexed_text[i*512:i*512+512])\n","del(indexed_text)\n","\n","dataset_tensor = torch.tensor(dataset_cut)\n","dataset_tensor.shape\n","\n","\n","\n","\n","# 这里使用 PyTorch 提供的 DataLoader() 构建训练集数据集表示，使用 TensorDataset() 构建训练集数据迭代器。\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# 构建数据集和数据迭代器，设定 batch_size 大小为 2\n","train_set = TensorDataset(dataset_tensor,\n","                          dataset_tensor)  # 标签与样本数据相同\n","train_loader = DataLoader(dataset=train_set,\n","                          batch_size=2,\n","                          shuffle=False)\n","print(train_loader)\n","\n","\n","# 检查是否机器有 GPU，如果有就在 GPU 运行，否则就在 CPU 运行。\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device: ', device)\n","\n","# 开始训练\n","from torch import nn\n","from torch.autograd import Variable\n","import time\n","\n","pre = time.time()\n","\n","epoch = 300  # 循环学习 30 次\n","\n","model.to(device)\n","model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 定义优化器\n","\n","for i in range(epoch):\n","    total_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data).to(device), Variable(\n","            target).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        loss, logits, _ = model(data, labels=target)\n","\n","        total_loss += loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx == len(train_loader)-1:\n","            # 在每个 Epoch 的最后输出一下结果\n","            print(i, 'average loss:', total_loss/len(train_loader))\n","\n","print('训练时间：', time.time()-pre)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:pytorch_transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (119010 > 1024). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"stream","text":["2972704\n","<torch.utils.data.dataloader.DataLoader object at 0x7fd7ac2919e8>\n","device:  cuda\n","0 average loss: tensor(1.5146, device='cuda:0', grad_fn=<DivBackward0>)\n","1 average loss: tensor(1.4992, device='cuda:0', grad_fn=<DivBackward0>)\n","2 average loss: tensor(1.4889, device='cuda:0', grad_fn=<DivBackward0>)\n","3 average loss: tensor(1.4771, device='cuda:0', grad_fn=<DivBackward0>)\n","4 average loss: tensor(1.4663, device='cuda:0', grad_fn=<DivBackward0>)\n","5 average loss: tensor(1.4555, device='cuda:0', grad_fn=<DivBackward0>)\n","6 average loss: tensor(1.4458, device='cuda:0', grad_fn=<DivBackward0>)\n","7 average loss: tensor(1.4365, device='cuda:0', grad_fn=<DivBackward0>)\n","8 average loss: tensor(1.4215, device='cuda:0', grad_fn=<DivBackward0>)\n","9 average loss: tensor(1.4130, device='cuda:0', grad_fn=<DivBackward0>)\n","10 average loss: tensor(1.4040, device='cuda:0', grad_fn=<DivBackward0>)\n","11 average loss: tensor(1.3914, device='cuda:0', grad_fn=<DivBackward0>)\n","12 average loss: tensor(1.3807, device='cuda:0', grad_fn=<DivBackward0>)\n","13 average loss: tensor(1.3704, device='cuda:0', grad_fn=<DivBackward0>)\n","14 average loss: tensor(1.3649, device='cuda:0', grad_fn=<DivBackward0>)\n","15 average loss: tensor(1.3524, device='cuda:0', grad_fn=<DivBackward0>)\n","16 average loss: tensor(1.3430, device='cuda:0', grad_fn=<DivBackward0>)\n","17 average loss: tensor(1.3318, device='cuda:0', grad_fn=<DivBackward0>)\n","18 average loss: tensor(1.3222, device='cuda:0', grad_fn=<DivBackward0>)\n","19 average loss: tensor(1.3113, device='cuda:0', grad_fn=<DivBackward0>)\n","20 average loss: tensor(1.3008, device='cuda:0', grad_fn=<DivBackward0>)\n","21 average loss: tensor(1.2946, device='cuda:0', grad_fn=<DivBackward0>)\n","22 average loss: tensor(1.2843, device='cuda:0', grad_fn=<DivBackward0>)\n","23 average loss: tensor(1.2723, device='cuda:0', grad_fn=<DivBackward0>)\n","24 average loss: tensor(1.2631, device='cuda:0', grad_fn=<DivBackward0>)\n","25 average loss: tensor(1.2526, device='cuda:0', grad_fn=<DivBackward0>)\n","26 average loss: tensor(1.2418, device='cuda:0', grad_fn=<DivBackward0>)\n","27 average loss: tensor(1.2342, device='cuda:0', grad_fn=<DivBackward0>)\n","28 average loss: tensor(1.2228, device='cuda:0', grad_fn=<DivBackward0>)\n","29 average loss: tensor(1.2149, device='cuda:0', grad_fn=<DivBackward0>)\n","30 average loss: tensor(1.2062, device='cuda:0', grad_fn=<DivBackward0>)\n","31 average loss: tensor(1.1955, device='cuda:0', grad_fn=<DivBackward0>)\n","32 average loss: tensor(1.1861, device='cuda:0', grad_fn=<DivBackward0>)\n","33 average loss: tensor(1.1756, device='cuda:0', grad_fn=<DivBackward0>)\n","34 average loss: tensor(1.1630, device='cuda:0', grad_fn=<DivBackward0>)\n","35 average loss: tensor(1.1565, device='cuda:0', grad_fn=<DivBackward0>)\n","36 average loss: tensor(1.1473, device='cuda:0', grad_fn=<DivBackward0>)\n","37 average loss: tensor(1.1399, device='cuda:0', grad_fn=<DivBackward0>)\n","38 average loss: tensor(1.1308, device='cuda:0', grad_fn=<DivBackward0>)\n","39 average loss: tensor(1.1194, device='cuda:0', grad_fn=<DivBackward0>)\n","40 average loss: tensor(1.1091, device='cuda:0', grad_fn=<DivBackward0>)\n","41 average loss: tensor(1.1007, device='cuda:0', grad_fn=<DivBackward0>)\n","42 average loss: tensor(1.0921, device='cuda:0', grad_fn=<DivBackward0>)\n","43 average loss: tensor(1.0801, device='cuda:0', grad_fn=<DivBackward0>)\n","44 average loss: tensor(1.0745, device='cuda:0', grad_fn=<DivBackward0>)\n","45 average loss: tensor(1.0677, device='cuda:0', grad_fn=<DivBackward0>)\n","46 average loss: tensor(1.0527, device='cuda:0', grad_fn=<DivBackward0>)\n","47 average loss: tensor(1.0515, device='cuda:0', grad_fn=<DivBackward0>)\n","48 average loss: tensor(1.0370, device='cuda:0', grad_fn=<DivBackward0>)\n","49 average loss: tensor(1.0311, device='cuda:0', grad_fn=<DivBackward0>)\n","50 average loss: tensor(1.0219, device='cuda:0', grad_fn=<DivBackward0>)\n","51 average loss: tensor(1.0121, device='cuda:0', grad_fn=<DivBackward0>)\n","52 average loss: tensor(1.0059, device='cuda:0', grad_fn=<DivBackward0>)\n","53 average loss: tensor(0.9933, device='cuda:0', grad_fn=<DivBackward0>)\n","54 average loss: tensor(0.9848, device='cuda:0', grad_fn=<DivBackward0>)\n","55 average loss: tensor(0.9770, device='cuda:0', grad_fn=<DivBackward0>)\n","56 average loss: tensor(0.9666, device='cuda:0', grad_fn=<DivBackward0>)\n","57 average loss: tensor(0.9616, device='cuda:0', grad_fn=<DivBackward0>)\n","58 average loss: tensor(0.9529, device='cuda:0', grad_fn=<DivBackward0>)\n","59 average loss: tensor(0.9409, device='cuda:0', grad_fn=<DivBackward0>)\n","60 average loss: tensor(0.9366, device='cuda:0', grad_fn=<DivBackward0>)\n","61 average loss: tensor(0.9269, device='cuda:0', grad_fn=<DivBackward0>)\n","62 average loss: tensor(0.9159, device='cuda:0', grad_fn=<DivBackward0>)\n","63 average loss: tensor(0.9102, device='cuda:0', grad_fn=<DivBackward0>)\n","64 average loss: tensor(0.9028, device='cuda:0', grad_fn=<DivBackward0>)\n","65 average loss: tensor(0.8924, device='cuda:0', grad_fn=<DivBackward0>)\n","66 average loss: tensor(0.8869, device='cuda:0', grad_fn=<DivBackward0>)\n","67 average loss: tensor(0.8750, device='cuda:0', grad_fn=<DivBackward0>)\n","68 average loss: tensor(0.8698, device='cuda:0', grad_fn=<DivBackward0>)\n","69 average loss: tensor(0.8617, device='cuda:0', grad_fn=<DivBackward0>)\n","70 average loss: tensor(0.8539, device='cuda:0', grad_fn=<DivBackward0>)\n","71 average loss: tensor(0.8464, device='cuda:0', grad_fn=<DivBackward0>)\n","72 average loss: tensor(0.8379, device='cuda:0', grad_fn=<DivBackward0>)\n","73 average loss: tensor(0.8268, device='cuda:0', grad_fn=<DivBackward0>)\n","74 average loss: tensor(0.8248, device='cuda:0', grad_fn=<DivBackward0>)\n","75 average loss: tensor(0.8124, device='cuda:0', grad_fn=<DivBackward0>)\n","76 average loss: tensor(0.8048, device='cuda:0', grad_fn=<DivBackward0>)\n","77 average loss: tensor(0.8005, device='cuda:0', grad_fn=<DivBackward0>)\n","78 average loss: tensor(0.7884, device='cuda:0', grad_fn=<DivBackward0>)\n","79 average loss: tensor(0.7831, device='cuda:0', grad_fn=<DivBackward0>)\n","80 average loss: tensor(0.7759, device='cuda:0', grad_fn=<DivBackward0>)\n","81 average loss: tensor(0.7654, device='cuda:0', grad_fn=<DivBackward0>)\n","82 average loss: tensor(0.7611, device='cuda:0', grad_fn=<DivBackward0>)\n","83 average loss: tensor(0.7561, device='cuda:0', grad_fn=<DivBackward0>)\n","84 average loss: tensor(0.7450, device='cuda:0', grad_fn=<DivBackward0>)\n","85 average loss: tensor(0.7387, device='cuda:0', grad_fn=<DivBackward0>)\n","86 average loss: tensor(0.7313, device='cuda:0', grad_fn=<DivBackward0>)\n","87 average loss: tensor(0.7209, device='cuda:0', grad_fn=<DivBackward0>)\n","88 average loss: tensor(0.7153, device='cuda:0', grad_fn=<DivBackward0>)\n","89 average loss: tensor(0.7100, device='cuda:0', grad_fn=<DivBackward0>)\n","90 average loss: tensor(0.7032, device='cuda:0', grad_fn=<DivBackward0>)\n","91 average loss: tensor(0.6946, device='cuda:0', grad_fn=<DivBackward0>)\n","92 average loss: tensor(0.6871, device='cuda:0', grad_fn=<DivBackward0>)\n","93 average loss: tensor(0.6778, device='cuda:0', grad_fn=<DivBackward0>)\n","94 average loss: tensor(0.6737, device='cuda:0', grad_fn=<DivBackward0>)\n","95 average loss: tensor(0.6653, device='cuda:0', grad_fn=<DivBackward0>)\n","96 average loss: tensor(0.6617, device='cuda:0', grad_fn=<DivBackward0>)\n","97 average loss: tensor(0.6557, device='cuda:0', grad_fn=<DivBackward0>)\n","98 average loss: tensor(0.6422, device='cuda:0', grad_fn=<DivBackward0>)\n","99 average loss: tensor(0.6392, device='cuda:0', grad_fn=<DivBackward0>)\n","100 average loss: tensor(0.6320, device='cuda:0', grad_fn=<DivBackward0>)\n","101 average loss: tensor(0.6248, device='cuda:0', grad_fn=<DivBackward0>)\n","102 average loss: tensor(0.6182, device='cuda:0', grad_fn=<DivBackward0>)\n","103 average loss: tensor(0.6090, device='cuda:0', grad_fn=<DivBackward0>)\n","104 average loss: tensor(0.6066, device='cuda:0', grad_fn=<DivBackward0>)\n","105 average loss: tensor(0.6019, device='cuda:0', grad_fn=<DivBackward0>)\n","106 average loss: tensor(0.5916, device='cuda:0', grad_fn=<DivBackward0>)\n","107 average loss: tensor(0.5856, device='cuda:0', grad_fn=<DivBackward0>)\n","108 average loss: tensor(0.5812, device='cuda:0', grad_fn=<DivBackward0>)\n","109 average loss: tensor(0.5786, device='cuda:0', grad_fn=<DivBackward0>)\n","110 average loss: tensor(0.5664, device='cuda:0', grad_fn=<DivBackward0>)\n","111 average loss: tensor(0.5631, device='cuda:0', grad_fn=<DivBackward0>)\n","112 average loss: tensor(0.5561, device='cuda:0', grad_fn=<DivBackward0>)\n","113 average loss: tensor(0.5502, device='cuda:0', grad_fn=<DivBackward0>)\n","114 average loss: tensor(0.5437, device='cuda:0', grad_fn=<DivBackward0>)\n","115 average loss: tensor(0.5390, device='cuda:0', grad_fn=<DivBackward0>)\n","116 average loss: tensor(0.5317, device='cuda:0', grad_fn=<DivBackward0>)\n","117 average loss: tensor(0.5243, device='cuda:0', grad_fn=<DivBackward0>)\n","118 average loss: tensor(0.5207, device='cuda:0', grad_fn=<DivBackward0>)\n","119 average loss: tensor(0.5186, device='cuda:0', grad_fn=<DivBackward0>)\n","120 average loss: tensor(0.5107, device='cuda:0', grad_fn=<DivBackward0>)\n","121 average loss: tensor(0.5054, device='cuda:0', grad_fn=<DivBackward0>)\n","122 average loss: tensor(0.4983, device='cuda:0', grad_fn=<DivBackward0>)\n","123 average loss: tensor(0.4935, device='cuda:0', grad_fn=<DivBackward0>)\n","124 average loss: tensor(0.4879, device='cuda:0', grad_fn=<DivBackward0>)\n","125 average loss: tensor(0.4781, device='cuda:0', grad_fn=<DivBackward0>)\n","126 average loss: tensor(0.4792, device='cuda:0', grad_fn=<DivBackward0>)\n","127 average loss: tensor(0.4739, device='cuda:0', grad_fn=<DivBackward0>)\n","128 average loss: tensor(0.4614, device='cuda:0', grad_fn=<DivBackward0>)\n","129 average loss: tensor(0.4603, device='cuda:0', grad_fn=<DivBackward0>)\n","130 average loss: tensor(0.4592, device='cuda:0', grad_fn=<DivBackward0>)\n","131 average loss: tensor(0.4504, device='cuda:0', grad_fn=<DivBackward0>)\n","132 average loss: tensor(0.4459, device='cuda:0', grad_fn=<DivBackward0>)\n","133 average loss: tensor(0.4401, device='cuda:0', grad_fn=<DivBackward0>)\n","134 average loss: tensor(0.4326, device='cuda:0', grad_fn=<DivBackward0>)\n","135 average loss: tensor(0.4270, device='cuda:0', grad_fn=<DivBackward0>)\n","136 average loss: tensor(0.4252, device='cuda:0', grad_fn=<DivBackward0>)\n","137 average loss: tensor(0.4195, device='cuda:0', grad_fn=<DivBackward0>)\n","138 average loss: tensor(0.4138, device='cuda:0', grad_fn=<DivBackward0>)\n","139 average loss: tensor(0.4104, device='cuda:0', grad_fn=<DivBackward0>)\n","140 average loss: tensor(0.4055, device='cuda:0', grad_fn=<DivBackward0>)\n","141 average loss: tensor(0.4003, device='cuda:0', grad_fn=<DivBackward0>)\n","142 average loss: tensor(0.3982, device='cuda:0', grad_fn=<DivBackward0>)\n","143 average loss: tensor(0.3925, device='cuda:0', grad_fn=<DivBackward0>)\n","144 average loss: tensor(0.3838, device='cuda:0', grad_fn=<DivBackward0>)\n","145 average loss: tensor(0.3806, device='cuda:0', grad_fn=<DivBackward0>)\n","146 average loss: tensor(0.3785, device='cuda:0', grad_fn=<DivBackward0>)\n","147 average loss: tensor(0.3709, device='cuda:0', grad_fn=<DivBackward0>)\n","148 average loss: tensor(0.3663, device='cuda:0', grad_fn=<DivBackward0>)\n","149 average loss: tensor(0.3639, device='cuda:0', grad_fn=<DivBackward0>)\n","150 average loss: tensor(0.3596, device='cuda:0', grad_fn=<DivBackward0>)\n","151 average loss: tensor(0.3576, device='cuda:0', grad_fn=<DivBackward0>)\n","152 average loss: tensor(0.3498, device='cuda:0', grad_fn=<DivBackward0>)\n","153 average loss: tensor(0.3465, device='cuda:0', grad_fn=<DivBackward0>)\n","154 average loss: tensor(0.3429, device='cuda:0', grad_fn=<DivBackward0>)\n","155 average loss: tensor(0.3384, device='cuda:0', grad_fn=<DivBackward0>)\n","156 average loss: tensor(0.3353, device='cuda:0', grad_fn=<DivBackward0>)\n","157 average loss: tensor(0.3299, device='cuda:0', grad_fn=<DivBackward0>)\n","158 average loss: tensor(0.3289, device='cuda:0', grad_fn=<DivBackward0>)\n","159 average loss: tensor(0.3241, device='cuda:0', grad_fn=<DivBackward0>)\n","160 average loss: tensor(0.3196, device='cuda:0', grad_fn=<DivBackward0>)\n","161 average loss: tensor(0.3135, device='cuda:0', grad_fn=<DivBackward0>)\n","162 average loss: tensor(0.3122, device='cuda:0', grad_fn=<DivBackward0>)\n","163 average loss: tensor(0.3067, device='cuda:0', grad_fn=<DivBackward0>)\n","164 average loss: tensor(0.3040, device='cuda:0', grad_fn=<DivBackward0>)\n","165 average loss: tensor(0.2990, device='cuda:0', grad_fn=<DivBackward0>)\n","166 average loss: tensor(0.2941, device='cuda:0', grad_fn=<DivBackward0>)\n","167 average loss: tensor(0.2929, device='cuda:0', grad_fn=<DivBackward0>)\n","168 average loss: tensor(0.2871, device='cuda:0', grad_fn=<DivBackward0>)\n","169 average loss: tensor(0.2871, device='cuda:0', grad_fn=<DivBackward0>)\n","170 average loss: tensor(0.2823, device='cuda:0', grad_fn=<DivBackward0>)\n","171 average loss: tensor(0.2804, device='cuda:0', grad_fn=<DivBackward0>)\n","172 average loss: tensor(0.2754, device='cuda:0', grad_fn=<DivBackward0>)\n","173 average loss: tensor(0.2713, device='cuda:0', grad_fn=<DivBackward0>)\n","174 average loss: tensor(0.2702, device='cuda:0', grad_fn=<DivBackward0>)\n","175 average loss: tensor(0.2630, device='cuda:0', grad_fn=<DivBackward0>)\n","176 average loss: tensor(0.2620, device='cuda:0', grad_fn=<DivBackward0>)\n","177 average loss: tensor(0.2623, device='cuda:0', grad_fn=<DivBackward0>)\n","178 average loss: tensor(0.2567, device='cuda:0', grad_fn=<DivBackward0>)\n","179 average loss: tensor(0.2551, device='cuda:0', grad_fn=<DivBackward0>)\n","180 average loss: tensor(0.2478, device='cuda:0', grad_fn=<DivBackward0>)\n","181 average loss: tensor(0.2465, device='cuda:0', grad_fn=<DivBackward0>)\n","182 average loss: tensor(0.2424, device='cuda:0', grad_fn=<DivBackward0>)\n","183 average loss: tensor(0.2403, device='cuda:0', grad_fn=<DivBackward0>)\n","184 average loss: tensor(0.2390, device='cuda:0', grad_fn=<DivBackward0>)\n","185 average loss: tensor(0.2355, device='cuda:0', grad_fn=<DivBackward0>)\n","186 average loss: tensor(0.2336, device='cuda:0', grad_fn=<DivBackward0>)\n","187 average loss: tensor(0.2290, device='cuda:0', grad_fn=<DivBackward0>)\n","188 average loss: tensor(0.2257, device='cuda:0', grad_fn=<DivBackward0>)\n","189 average loss: tensor(0.2233, device='cuda:0', grad_fn=<DivBackward0>)\n","190 average loss: tensor(0.2202, device='cuda:0', grad_fn=<DivBackward0>)\n","191 average loss: tensor(0.2202, device='cuda:0', grad_fn=<DivBackward0>)\n","192 average loss: tensor(0.2146, device='cuda:0', grad_fn=<DivBackward0>)\n","193 average loss: tensor(0.2144, device='cuda:0', grad_fn=<DivBackward0>)\n","194 average loss: tensor(0.2127, device='cuda:0', grad_fn=<DivBackward0>)\n","195 average loss: tensor(0.2070, device='cuda:0', grad_fn=<DivBackward0>)\n","196 average loss: tensor(0.2033, device='cuda:0', grad_fn=<DivBackward0>)\n","197 average loss: tensor(0.2031, device='cuda:0', grad_fn=<DivBackward0>)\n","198 average loss: tensor(0.2019, device='cuda:0', grad_fn=<DivBackward0>)\n","199 average loss: tensor(0.2000, device='cuda:0', grad_fn=<DivBackward0>)\n","200 average loss: tensor(0.1966, device='cuda:0', grad_fn=<DivBackward0>)\n","201 average loss: tensor(0.1954, device='cuda:0', grad_fn=<DivBackward0>)\n","202 average loss: tensor(0.1914, device='cuda:0', grad_fn=<DivBackward0>)\n","203 average loss: tensor(0.1874, device='cuda:0', grad_fn=<DivBackward0>)\n","204 average loss: tensor(0.1869, device='cuda:0', grad_fn=<DivBackward0>)\n","205 average loss: tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>)\n","206 average loss: tensor(0.1802, device='cuda:0', grad_fn=<DivBackward0>)\n","207 average loss: tensor(0.1815, device='cuda:0', grad_fn=<DivBackward0>)\n","208 average loss: tensor(0.1796, device='cuda:0', grad_fn=<DivBackward0>)\n","209 average loss: tensor(0.1751, device='cuda:0', grad_fn=<DivBackward0>)\n","210 average loss: tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>)\n","211 average loss: tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>)\n","212 average loss: tensor(0.1703, device='cuda:0', grad_fn=<DivBackward0>)\n","214 average loss: tensor(0.1647, device='cuda:0', grad_fn=<DivBackward0>)\n","215 average loss: tensor(0.1634, device='cuda:0', grad_fn=<DivBackward0>)\n","216 average loss: tensor(0.1628, device='cuda:0', grad_fn=<DivBackward0>)\n","217 average loss: tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>)\n","218 average loss: tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>)\n","219 average loss: tensor(0.1541, device='cuda:0', grad_fn=<DivBackward0>)\n","220 average loss: tensor(0.1536, device='cuda:0', grad_fn=<DivBackward0>)\n","221 average loss: tensor(0.1517, device='cuda:0', grad_fn=<DivBackward0>)\n","222 average loss: tensor(0.1487, device='cuda:0', grad_fn=<DivBackward0>)\n","223 average loss: tensor(0.1469, device='cuda:0', grad_fn=<DivBackward0>)\n","224 average loss: tensor(0.1469, device='cuda:0', grad_fn=<DivBackward0>)\n","225 average loss: tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>)\n","226 average loss: tensor(0.1452, device='cuda:0', grad_fn=<DivBackward0>)\n","227 average loss: tensor(0.1431, device='cuda:0', grad_fn=<DivBackward0>)\n","228 average loss: tensor(0.1434, device='cuda:0', grad_fn=<DivBackward0>)\n","229 average loss: tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>)\n","230 average loss: tensor(0.1376, device='cuda:0', grad_fn=<DivBackward0>)\n","231 average loss: tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>)\n","232 average loss: tensor(0.1329, device='cuda:0', grad_fn=<DivBackward0>)\n","233 average loss: tensor(0.1316, device='cuda:0', grad_fn=<DivBackward0>)\n","234 average loss: tensor(0.1345, device='cuda:0', grad_fn=<DivBackward0>)\n","235 average loss: tensor(0.1299, device='cuda:0', grad_fn=<DivBackward0>)\n","236 average loss: tensor(0.1284, device='cuda:0', grad_fn=<DivBackward0>)\n","237 average loss: tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>)\n","238 average loss: tensor(0.1250, device='cuda:0', grad_fn=<DivBackward0>)\n","239 average loss: tensor(0.1232, device='cuda:0', grad_fn=<DivBackward0>)\n","240 average loss: tensor(0.1212, device='cuda:0', grad_fn=<DivBackward0>)\n","241 average loss: tensor(0.1212, device='cuda:0', grad_fn=<DivBackward0>)\n","242 average loss: tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>)\n","243 average loss: tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>)\n","244 average loss: tensor(0.1197, device='cuda:0', grad_fn=<DivBackward0>)\n","245 average loss: tensor(0.1147, device='cuda:0', grad_fn=<DivBackward0>)\n","246 average loss: tensor(0.1152, device='cuda:0', grad_fn=<DivBackward0>)\n","247 average loss: tensor(0.1114, device='cuda:0', grad_fn=<DivBackward0>)\n","248 average loss: tensor(0.1117, device='cuda:0', grad_fn=<DivBackward0>)\n","249 average loss: tensor(0.1104, device='cuda:0', grad_fn=<DivBackward0>)\n","250 average loss: tensor(0.1094, device='cuda:0', grad_fn=<DivBackward0>)\n","251 average loss: tensor(0.1069, device='cuda:0', grad_fn=<DivBackward0>)\n","252 average loss: tensor(0.1082, device='cuda:0', grad_fn=<DivBackward0>)\n","253 average loss: tensor(0.1074, device='cuda:0', grad_fn=<DivBackward0>)\n","254 average loss: tensor(0.1072, device='cuda:0', grad_fn=<DivBackward0>)\n","255 average loss: tensor(0.1034, device='cuda:0', grad_fn=<DivBackward0>)\n","256 average loss: tensor(0.1044, device='cuda:0', grad_fn=<DivBackward0>)\n","257 average loss: tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>)\n","258 average loss: tensor(0.1002, device='cuda:0', grad_fn=<DivBackward0>)\n","259 average loss: tensor(0.0986, device='cuda:0', grad_fn=<DivBackward0>)\n","260 average loss: tensor(0.0998, device='cuda:0', grad_fn=<DivBackward0>)\n","261 average loss: tensor(0.0988, device='cuda:0', grad_fn=<DivBackward0>)\n","262 average loss: tensor(0.0981, device='cuda:0', grad_fn=<DivBackward0>)\n","263 average loss: tensor(0.0956, device='cuda:0', grad_fn=<DivBackward0>)\n","264 average loss: tensor(0.0930, device='cuda:0', grad_fn=<DivBackward0>)\n","265 average loss: tensor(0.0952, device='cuda:0', grad_fn=<DivBackward0>)\n","266 average loss: tensor(0.0939, device='cuda:0', grad_fn=<DivBackward0>)\n","267 average loss: tensor(0.0932, device='cuda:0', grad_fn=<DivBackward0>)\n","268 average loss: tensor(0.0928, device='cuda:0', grad_fn=<DivBackward0>)\n","269 average loss: tensor(0.0894, device='cuda:0', grad_fn=<DivBackward0>)\n","270 average loss: tensor(0.0897, device='cuda:0', grad_fn=<DivBackward0>)\n","271 average loss: tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>)\n","272 average loss: tensor(0.0885, device='cuda:0', grad_fn=<DivBackward0>)\n","273 average loss: tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>)\n","274 average loss: tensor(0.0871, device='cuda:0', grad_fn=<DivBackward0>)\n","275 average loss: tensor(0.0863, device='cuda:0', grad_fn=<DivBackward0>)\n","276 average loss: tensor(0.0841, device='cuda:0', grad_fn=<DivBackward0>)\n","277 average loss: tensor(0.0823, device='cuda:0', grad_fn=<DivBackward0>)\n","278 average loss: tensor(0.0844, device='cuda:0', grad_fn=<DivBackward0>)\n","279 average loss: tensor(0.0811, device='cuda:0', grad_fn=<DivBackward0>)\n","280 average loss: tensor(0.0798, device='cuda:0', grad_fn=<DivBackward0>)\n","281 average loss: tensor(0.0792, device='cuda:0', grad_fn=<DivBackward0>)\n","282 average loss: tensor(0.0795, device='cuda:0', grad_fn=<DivBackward0>)\n","283 average loss: tensor(0.0789, device='cuda:0', grad_fn=<DivBackward0>)\n","284 average loss: tensor(0.0784, device='cuda:0', grad_fn=<DivBackward0>)\n","285 average loss: tensor(0.0765, device='cuda:0', grad_fn=<DivBackward0>)\n","286 average loss: tensor(0.0759, device='cuda:0', grad_fn=<DivBackward0>)\n","287 average loss: tensor(0.0763, device='cuda:0', grad_fn=<DivBackward0>)\n","288 average loss: tensor(0.0769, device='cuda:0', grad_fn=<DivBackward0>)\n","289 average loss: tensor(0.0760, device='cuda:0', grad_fn=<DivBackward0>)\n","290 average loss: tensor(0.0729, device='cuda:0', grad_fn=<DivBackward0>)\n","291 average loss: tensor(0.0746, device='cuda:0', grad_fn=<DivBackward0>)\n","292 average loss: tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>)\n","293 average loss: tensor(0.0725, device='cuda:0', grad_fn=<DivBackward0>)\n","294 average loss: tensor(0.0705, device='cuda:0', grad_fn=<DivBackward0>)\n","295 average loss: tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>)\n","296 average loss: tensor(0.0696, device='cuda:0', grad_fn=<DivBackward0>)\n","297 average loss: tensor(0.0694, device='cuda:0', grad_fn=<DivBackward0>)\n","298 average loss: tensor(0.0696, device='cuda:0', grad_fn=<DivBackward0>)\n","299 average loss: tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n","训练时间： 7879.691942453384\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"oCYdjtiBZXhN","colab_type":"code","colab":{}},"source":["# 训练结束后，可以使模型生成文本，观察输出。\n","text = \"秦羽从府邸中出来，急速向南飞去\"  # 这里也可以输入不同的英文文本"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nL4VLPtwhL78","colab_type":"code","outputId":"490ace60-f38c-4e16-91a4-a2197335e66c","executionInfo":{"status":"ok","timestamp":1586546760219,"user_tz":-480,"elapsed":25282,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model.eval()\n","total_predicted_text = text\n","\n","# 使训练后的模型进行 500 次预测\n","for _ in range(500):\n","    tokens_tensor = tokens_tensor.to('cuda')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","\n","    if len(indexed_tokens) > 1023:\n","        # 模型最长输入长度为1024，如果长度过长则截断\n","        indexed_tokens = indexed_tokens[-1023:]\n","\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["秦羽从府邸中出来，急速向南飞去��������������� �������������������������������。������������。大�����������������������。\n","�����������������的����������������一�����������上���������������������不����……�������一����������生。��������大������一����之���������三�����������天天��������天生��������……��������大�����一�����������������……�������一�����������������������������的�����������������……\n","\n","------------------------\n","\n","\n","�������~~~~���������之������~~��������一�����������������������������������������之����人��������~~ST�����������……\n","��������������一�������的����\n","\n","��\n","\n","�� ��������不��\n","\n","�����一�������\n"],"name":"stdout"}]}]}
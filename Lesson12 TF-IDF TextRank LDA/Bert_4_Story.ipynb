{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"Bert_4_Story.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"pycharm":{"name":"#%% md\n"},"id":"662f40saZXgy","colab_type":"text"},"source":["https://www.cnblogs.com/wwj99/p/12503545.html\n","BPE 算法\n","\n","GPT-2 模型在数据预处理时使用了字节对编码（Byte Pair Encoding，简称 BPE）方法，BPE 是一种能够解决未登录词问题，并减小词典大小的方法。它综合利用了单词层面编码和字符层面编码的优势，举例来说，我们要对下面的字符串编码，\n","\n","aaabdaaabac\n","字节对 aa 出现的次数最多，所以我们将它替换成一个没在字符串中被用过的字符 Z ，\n","\n","ZabdZabac\n","Z=aa\n","然后我们重复这个过程，用 Y 替换 ab ，\n","\n","ZYdZYac\n","Y=ab\n","Z=aa\n","继续，用 X 替换 ZY ，\n","\n","XdXac\n","X=ZY\n","Y=ab\n","Z=aa"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"NGVCpgE1ZXgz","colab_type":"code","outputId":"72edc04c-d785-4505-ffdb-ab048e3ccb24","executionInfo":{"status":"ok","timestamp":1586583667849,"user_tz":-480,"elapsed":1237,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["import re\n","import collections\n","\n","def get_stats(vocab):\n","    pairs = collections.defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols)-1):\n","            pairs[symbols[i], symbols[i+1]] += freq  # 计算字节对出现频率\n","    return pairs\n","\n","\n","def merge_vocab(pair, v_in):\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))  # 将字节对中可解释为正则运算符的字符转义\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # 将要合并的字节对前后只能为空白字符\n","    for word in v_in:\n","        w_out = p.sub(''.join(pair), word)  # 合并符合条件的字节对\n","        v_out[w_out] = v_in[word]\n","    return v_out\n","\n","vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,\n","         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n","num_merges = 10\n","for i in range(num_merges):\n","    pairs = get_stats(vocab)\n","    best = max(pairs, key=pairs.get)  # 选择频率最大的字节对\n","    vocab = merge_vocab(best, vocab)\n","    print(best)\n","print(vocab)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["('e', 's')\n","('es', 't')\n","('est', '</w>')\n","('l', 'o')\n","('lo', 'w')\n","('n', 'e')\n","('ne', 'w')\n","('new', 'est</w>')\n","('low', '</w>')\n","('w', 'i')\n","{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"tiY7_pDBZXg4","colab_type":"code","colab":{}},"source":["# top-k\n","import random\n","\n","def select_top_k(predictions, k=10):\n","    predicted_index = random.choice(\n","        predictions[0, -1, :].sort(descending=True)[1][:10]).item()\n","    return predicted_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"sd2xKkwJZXg7","colab_type":"code","outputId":"6bc8d3cf-501c-4c8e-e409-b3cef9fa19f7","executionInfo":{"status":"ok","timestamp":1586583672230,"user_tz":-480,"elapsed":5603,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":449}},"source":["# 使用在 PyTorch-Transformers 模型库中封装好的 GPT2Tokenizer() 和 GPT2LMHeadModel()\n","# 安装 PyTorch-Transformers\n","!pip install pytorch_transformers==1.0\n","# !pip install pytorch_transformers==1.0 -i  https://pypi.tuna.tsinghua.edu.cn/simple/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pytorch_transformers==1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n","\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.12.38)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.18.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (4.38.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers==1.0) (1.4.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers==1.0) (1.15.38)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers==1.0) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->pytorch_transformers==1.0) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->pytorch_transformers==1.0) (0.15.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.38->boto3->pytorch_transformers==1.0) (1.12.0)\n","Installing collected packages: sentencepiece, pytorch-transformers\n","Successfully installed pytorch-transformers-1.0.0 sentencepiece-0.1.85\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W7UHdFnymh9o","colab_type":"code","colab":{}},"source":["# https://huggingface.co/transformers/pretrained_models.html?highlight=bert%20base\n","# bert-base-chinese"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"L56M6lHfZXg9","colab_type":"code","outputId":"414ec485-5fd3-40e0-ef13-890d6e04ffd8","executionInfo":{"status":"ok","timestamp":1586584983464,"user_tz":-480,"elapsed":3972,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["import torch\n","from pytorch_transformers import BertForNextSentencePrediction as Model, BertTokenizer as Tokenizer\n","# from pytorch_transformers import BertModel as Model, BertTokenizer as Tokenizer\n","# from pytorch_transformers import GPT2LMHeadModel as Model, GPT2Tokenizer as Tokenizer\n","# pretrainedDataStr = 'gpt2'\n","pretrainedDataStr = 'bert-base-chinese'\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# 载入预训练模型的分词器\n","tokenizer = Tokenizer.from_pretrained(pretrainedDataStr)\n","\n","# 读取预训练模型\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model = Model.from_pretrained(pretrainedDataStr)\n","\n","# 使用 Tokenizer 对输入进行编码\n","# text = \"Yesterday, a man named Jack said he saw an alien,\"\n","text = \"昨天, 一个名叫杰克的人说他看到了一头狮子,\"\n","indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","tokens_tensor.shape"],"execution_count":20,"outputs":[{"output_type":"stream","text":["INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n","INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/pytorch_transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n","INFO:pytorch_transformers.modeling_utils:Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 21128\n","}\n","\n","INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n","INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 21])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"id":"iogM1riQZXhA","colab_type":"code","outputId":"58ce95d5-f37a-4aeb-f670-3bd0ef334b84","executionInfo":{"status":"ok","timestamp":1586583928814,"user_tz":-480,"elapsed":721,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(indexed_tokens)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[3220, 1922, 117, 672, 703, 1400, 1374, 3346, 1047, 4639, 783, 6433, 801, 4693, 1169, 750, 672, 1929, 4327, 2095, 117]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"cbK2k_QcZXhD","colab_type":"code","outputId":"4d7063d0-b2ce-49b1-e8c4-802e27134072","executionInfo":{"status":"ok","timestamp":1586584987589,"user_tz":-480,"elapsed":1385,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["model.eval()\n","total_predicted_text = text\n","n = 100  # 预测过程的循环次数\n","for _ in range(n):\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        print(outputs)\n","        predictions = outputs[0]\n","        print(outputs[0])\n","        # loss, logits = outputs[:2]\n","        break\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["(tensor([[ 2.2125, -1.0247]]),)\n","tensor([[ 2.2125, -1.0247]])\n","昨天, 一个名叫杰克的人说他看到了一头狮子,\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"nkakEo03ZXhG","colab_type":"text"},"source":["微调生成戏剧文本\n","接下来，我们将使用一些戏剧剧本对 GPT-2 进行微调。由于 OpenAI 团队开源的 GPT-2 模型预训练参数为使用英文数据集预训练后得到的，虽然可以在微调时使用中文数据集，但需要大量数据和时间才会有好的效果，所以这里我们使用了英文数据集进行微调，从而更好地展现 GPT-2 模型的能力。\n","\n","首先，下载训练数据集，这里使用了莎士比亚的戏剧作品《罗密欧与朱丽叶》作为训练样本。数据集已经提前下载好并放在云盘中，链接：https://pan.baidu.com/s/1LiTgiake1KC8qptjRncJ5w 提取码：km06"]},{"cell_type":"code","metadata":{"id":"fS8RpP9UaMX2","colab_type":"code","outputId":"dc93cf75-19df-48dd-9e15-f285b1ebe1c8","executionInfo":{"status":"ok","timestamp":1586583912033,"user_tz":-480,"elapsed":245362,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/Colab Notebooks/Lesson12"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/Lesson12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"9tpWJ3D_ZXhK","colab_type":"code","outputId":"a83b3a24-b01b-47c5-d73c-888570878c39","executionInfo":{"status":"error","timestamp":1586583039272,"user_tz":-480,"elapsed":2725,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":443}},"source":["# with open('./Datas/romeo_and_juliet.txt', 'r') as f:星辰变\n","with open('./Datas/星辰变.txt', 'r') as f:\n","    dataset = f.read()\n","\n","print(len(dataset))\n","\n","dataset = dataset[:len(dataset)//50]\n","\n","# 预处理训练集，将训练集编码、分段。\n","indexed_text = tokenizer.encode(dataset)\n","del(dataset)\n","\n","dataset_cut = []\n","for i in range(len(indexed_text)//512):\n","    # 将字符串分段成长度为 512\n","    dataset_cut.append(indexed_text[i*512:i*512+512])\n","del(indexed_text)\n","\n","dataset_tensor = torch.tensor(dataset_cut)\n","dataset_tensor.shape\n","\n","\n","\n","\n","# 这里使用 PyTorch 提供的 DataLoader() 构建训练集数据集表示，使用 TensorDataset() 构建训练集数据迭代器。\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# 构建数据集和数据迭代器，设定 batch_size 大小为 2\n","train_set = TensorDataset(dataset_tensor,\n","                          dataset_tensor)  # 标签与样本数据相同\n","train_loader = DataLoader(dataset=train_set,\n","                          batch_size=2,\n","                          shuffle=False)\n","print(train_loader)\n","\n","\n","# 检查是否机器有 GPU，如果有就在 GPU 运行，否则就在 CPU 运行。\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device: ', device)\n","\n","# 开始训练\n","from torch import nn\n","from torch.autograd import Variable\n","import time\n","\n","pre = time.time()\n","\n","epoch = 30  # 循环学习 30 次\n","\n","model.to(device)\n","model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 定义优化器\n","\n","for i in range(epoch):\n","    total_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data).to(device), Variable(\n","            target).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        loss, logits, _ = model(data, labels=target)\n","\n","        total_loss += loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx == len(train_loader)-1:\n","            # 在每个 Epoch 的最后输出一下结果\n","            print(i, 'average loss:', total_loss/len(train_loader))\n","\n","print('训练时间：', time.time()-pre)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2972704\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:pytorch_transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (52948 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x7f0bd83f2b38>\n","device:  cuda\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a83b04accbc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"]}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"oCYdjtiBZXhN","colab_type":"code","outputId":"c65dbdc2-b036-4876-e2c2-a011edc50d0e","executionInfo":{"status":"error","timestamp":1586574728488,"user_tz":-480,"elapsed":1245,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["# 训练结束后，可以使模型生成文本，观察输出。\n","text = \"秦羽从府邸中出来，急速向南飞去\"  # 这里也可以输入不同的英文文本\n","indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model.eval()\n","total_predicted_text = text\n","\n","# 使训练后的模型进行 500 次预测\n","for _ in range(500):\n","    tokens_tensor = tokens_tensor.to('cuda')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","\n","    if len(indexed_tokens) > 1023:\n","        # 模型最长输入长度为1024，如果长度过长则截断\n","        indexed_tokens = indexed_tokens[-1023:]\n","\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ef2bb4de93ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"秦羽从府邸中出来，急速向南飞去\"\u001b[0m  \u001b[0;31m# 这里也可以输入不同的英文文本\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"nL4VLPtwhL78","colab_type":"code","outputId":"490ace60-f38c-4e16-91a4-a2197335e66c","executionInfo":{"status":"ok","timestamp":1586546760219,"user_tz":-480,"elapsed":25282,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model.eval()\n","total_predicted_text = text\n","\n","# 使训练后的模型进行 500 次预测\n","for _ in range(500):\n","    tokens_tensor = tokens_tensor.to('cuda')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","\n","    if len(indexed_tokens) > 1023:\n","        # 模型最长输入长度为1024，如果长度过长则截断\n","        indexed_tokens = indexed_tokens[-1023:]\n","\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["秦羽从府邸中出来，急速向南飞去��������������� �������������������������������。������������。大�����������������������。\n","�����������������的����������������一�����������上���������������������不����……�������一����������生。��������大������一����之���������三�����������天天��������天生��������……��������大�����一�����������������……�������一�����������������������������的�����������������……\n","\n","------------------------\n","\n","\n","�������~~~~���������之������~~��������一�����������������������������������������之����人��������~~ST�����������……\n","��������������一�������的����\n","\n","��\n","\n","�� ��������不��\n","\n","�����一�������\n"],"name":"stdout"}]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import logging\n",
    "import os, json, copy\n",
    "from typing import Dict, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "from transformers.file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "CONFIG_NAME = \"config.json\"\n",
    "S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n",
    "CLOUDFRONT_DISTRIB_PREFIX = \"https://d2ws9o8vfrpkyk.cloudfront.net\"\n",
    "\n",
    "try:\n",
    "    USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "    USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "    if USE_TORCH in (\"1\", \"ON\", \"YES\", \"AUTO\") and USE_TF not in (\"1\", \"ON\", \"YES\"):\n",
    "        import torch\n",
    "\n",
    "        _torch_available = True  # pylint: disable=invalid-name\n",
    "        logger.info(\"PyTorch version {} available.\".format(torch.__version__))\n",
    "    else:\n",
    "        logger.info(\"Disabling PyTorch because USE_TF is set\")\n",
    "        _torch_available = False\n",
    "except ImportError:\n",
    "    _torch_available = False  # pylint: disable=invalid-name\n",
    "\n",
    "try:\n",
    "    USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "    USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "\n",
    "    if USE_TF in (\"1\", \"ON\", \"YES\", \"AUTO\") and USE_TORCH not in (\"1\", \"ON\", \"YES\"):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        assert hasattr(tf, \"__version__\") and int(tf.__version__[0]) >= 2\n",
    "        _tf_available = True  # pylint: disable=invalid-name\n",
    "        logger.info(\"TensorFlow version {} available.\".format(tf.__version__))\n",
    "    else:\n",
    "        logger.info(\"Disabling Tensorflow because USE_TORCH is set\")\n",
    "        _tf_available = False\n",
    "except (ImportError, AssertionError):\n",
    "    _tf_available = False  # pylint: disable=invalid-name\n",
    "\n",
    "try:\n",
    "    from torch.hub import _get_torch_home\n",
    "\n",
    "    torch_cache_home = _get_torch_home()\n",
    "except ImportError:\n",
    "    torch_cache_home = os.path.expanduser(\n",
    "        os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n",
    "    )\n",
    "default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n",
    "\n",
    "try:\n",
    "    from pathlib import Path\n",
    "\n",
    "    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
    "        os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path))\n",
    "    )\n",
    "except (AttributeError, ImportError):\n",
    "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\n",
    "        \"PYTORCH_TRANSFORMERS_CACHE\", os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n",
    "    )\n",
    "\n",
    "PYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
    "TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
    "\n",
    "def is_remote_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\", \"s3\")\n",
    "\n",
    "\n",
    "def hf_bucket_url(identifier, postfix=None, cdn=False) -> str:\n",
    "    endpoint = CLOUDFRONT_DISTRIB_PREFIX if cdn else S3_BUCKET_PREFIX\n",
    "    if postfix is None:\n",
    "        return \"/\".join((endpoint, identifier))\n",
    "    else:\n",
    "        return \"/\".join((endpoint, identifier, postfix))\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "class PretrainedConfig(object):\n",
    "    pretrained_config_archive_map = {}  # type: Dict[str, str]\n",
    "    model_type = \"\"  # type: str\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Attributes with defaults\n",
    "        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n",
    "        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n",
    "        self.output_past = kwargs.pop(\"output_past\", True)  # Not used by all models\n",
    "        self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n",
    "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
    "        self.pruned_heads = kwargs.pop(\"pruned_heads\", {})\n",
    "\n",
    "        # Is decoder is used in encoder-decoder models to differentiate encoder from decoder\n",
    "        self.is_encoder_decoder = kwargs.pop(\"is_encoder_decoder\", False)\n",
    "        self.is_decoder = kwargs.pop(\"is_decoder\", False)\n",
    "\n",
    "        # Parameters for sequence generation\n",
    "        self.max_length = kwargs.pop(\"max_length\", 20)\n",
    "        self.min_length = kwargs.pop(\"min_length\", 0)\n",
    "        self.do_sample = kwargs.pop(\"do_sample\", False)\n",
    "        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n",
    "        self.num_beams = kwargs.pop(\"num_beams\", 1)\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.top_k = kwargs.pop(\"top_k\", 50)\n",
    "        self.top_p = kwargs.pop(\"top_p\", 1.0)\n",
    "        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n",
    "        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n",
    "        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n",
    "        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n",
    "        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n",
    "\n",
    "        # Fine-tuning task arguments\n",
    "        self.architectures = kwargs.pop(\"architectures\", None)\n",
    "        self.finetuning_task = kwargs.pop(\"finetuning_task\", None)\n",
    "        self.num_labels = kwargs.pop(\"num_labels\", 2)\n",
    "        self.id2label = kwargs.pop(\"id2label\", {i: \"LABEL_{}\".format(i) for i in range(self.num_labels)})\n",
    "        self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n",
    "        self.label2id = kwargs.pop(\"label2id\", dict(zip(self.id2label.values(), self.id2label.keys())))\n",
    "        self.label2id = dict((key, int(value)) for key, value in self.label2id.items())\n",
    "\n",
    "        # Tokenizer arguments TODO: eventually tokenizer and models should share the same config\n",
    "        self.prefix = kwargs.pop(\"prefix\", None)\n",
    "        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n",
    "        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n",
    "        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n",
    "        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n",
    "\n",
    "        # task specific arguments\n",
    "        self.task_specific_params = kwargs.pop(\"task_specific_params\", None)\n",
    "\n",
    "        # Additional attributes without default values\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                setattr(self, key, value)\n",
    "            except AttributeError as err:\n",
    "                logger.error(\"Can't set {} with value {} for {}\".format(key, value, self))\n",
    "                raise err    \n",
    "            \n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return self._num_labels\n",
    "    \n",
    "    @num_labels.setter\n",
    "    def num_labels(self, num_labels):\n",
    "        self._num_labels = num_labels\n",
    "        self.id2label = {i: \"LABEL_{}\".format(i) for i in range(self.num_labels)}\n",
    "        self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n",
    "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
    "        self.label2id = dict((key, int(value)) for key, value in self.label2id.items())\n",
    "        \n",
    "        \n",
    "    def save_pretrained(self, save_directory):\n",
    "        assert os.path.isdir(\n",
    "            save_directory\n",
    "        ), \"Saving path should be a directory where the model and configuration can be saved\"\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n",
    "\n",
    "        self.to_json_file(output_config_file)\n",
    "        logger.info(\"Configuration saved in {}\".format(output_config_file))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs) -> \"PretrainedConfig\":\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "        return cls.from_dict(config_dict, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_config_dict(\n",
    "        cls, pretrained_model_name_or_path: str, pretrained_config_archive_map: Optional[Dict] = None, **kwargs\n",
    "    ) -> Tuple[Dict, Dict]:\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "\n",
    "        if pretrained_config_archive_map is None:\n",
    "            pretrained_config_archive_map = cls.pretrained_config_archive_map\n",
    "\n",
    "        if pretrained_model_name_or_path in pretrained_config_archive_map:\n",
    "            config_file = pretrained_config_archive_map[pretrained_model_name_or_path]\n",
    "        elif os.path.isdir(pretrained_model_name_or_path):\n",
    "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
    "        elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "            config_file = pretrained_model_name_or_path\n",
    "        else:\n",
    "            config_file = hf_bucket_url(pretrained_model_name_or_path, postfix=CONFIG_NAME)\n",
    "\n",
    "        try:\n",
    "            # Load from URL or cache if already cached\n",
    "            resolved_config_file = cached_path(\n",
    "                config_file,\n",
    "                cache_dir=cache_dir,\n",
    "                force_download=force_download,\n",
    "                proxies=proxies,\n",
    "                resume_download=resume_download,\n",
    "                local_files_only=local_files_only,\n",
    "            )\n",
    "            # Load config dict\n",
    "            if resolved_config_file is None:\n",
    "                raise EnvironmentError\n",
    "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
    "\n",
    "        except EnvironmentError:\n",
    "            if pretrained_model_name_or_path in pretrained_config_archive_map:\n",
    "                msg = \"Couldn't reach server at '{}' to download pretrained model configuration file.\".format(\n",
    "                    config_file\n",
    "                )\n",
    "            else:\n",
    "                msg = (\n",
    "                    \"Can't load '{}'. Make sure that:\\n\\n\"\n",
    "                    \"- '{}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    \"- or '{}' is the correct path to a directory containing a '{}' file\\n\\n\".format(\n",
    "                        pretrained_model_name_or_path,\n",
    "                        pretrained_model_name_or_path,\n",
    "                        pretrained_model_name_or_path,\n",
    "                        CONFIG_NAME,\n",
    "                    )\n",
    "                )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            msg = (\n",
    "                \"Couldn't reach server at '{}' to download configuration file or \"\n",
    "                \"configuration file is not a valid JSON file. \"\n",
    "                \"Please check network or file content here: {}.\".format(config_file, resolved_config_file)\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        if resolved_config_file == config_file:\n",
    "            logger.info(\"loading configuration file {}\".format(config_file))\n",
    "        else:\n",
    "            logger.info(\"loading configuration file {} from cache at {}\".format(config_file, resolved_config_file))\n",
    "\n",
    "        return config_dict, kwargs\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict, **kwargs) -> \"PretrainedConfig\":\n",
    "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
    "\n",
    "        config = cls(**config_dict)\n",
    "\n",
    "        if hasattr(config, \"pruned_heads\"):\n",
    "            config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items())\n",
    "\n",
    "        # Update config with kwargs if needed\n",
    "        to_remove = []\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "                to_remove.append(key)\n",
    "        for key in to_remove:\n",
    "            kwargs.pop(key, None)\n",
    "\n",
    "        logger.info(\"Model config %s\", str(config))\n",
    "        if return_unused_kwargs:\n",
    "            return config, kwargs\n",
    "        else:\n",
    "            return config \n",
    "        \n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file: str) -> \"PretrainedConfig\":\n",
    "        \"\"\"\n",
    "        Constructs a `Config` from the path to a json file of parameters.\n",
    "\n",
    "        Args:\n",
    "            json_file (:obj:`string`):\n",
    "                Path to the JSON file containing the parameters.\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: An instance of a configuration object\n",
    "\n",
    "        \"\"\"\n",
    "        config_dict = cls._dict_from_json_file(json_file)\n",
    "        return cls(**config_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def _dict_from_json_file(cls, json_file: str):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "            text = reader.read()\n",
    "        return json.loads(text)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"{} {}\".format(self.__class__.__name__, self.to_json_string())\n",
    "    \n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        if hasattr(self.__class__, \"model_type\"):\n",
    "            output[\"model_type\"] = self.__class__.model_type\n",
    "        return output\n",
    "    \n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "    \n",
    "    \n",
    "    def to_json_file(self, json_file_path):\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(self.to_json_string())\n",
    "    \n",
    "    def update(self, config_dict: Dict):\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"albert-base-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-config.json\",\n",
    "    \"albert-large-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-config.json\",\n",
    "    \"albert-xlarge-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-config.json\",\n",
    "    \"albert-xxlarge-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-config.json\",\n",
    "    \"albert-base-v2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-config.json\",\n",
    "    \"albert-large-v2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json\",\n",
    "    \"albert-xlarge-v2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v2-config.json\",\n",
    "    \"albert-xxlarge-v2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-config.json\",\n",
    "}\n",
    "\n",
    "class AlbertConfig(PretrainedConfig):\n",
    "    pretrained_config_archive_map = ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
    "    model_type = \"albert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30000,\n",
    "        embedding_size=128,\n",
    "        hidden_size=4096,\n",
    "        num_hidden_layers=12,\n",
    "        num_hidden_groups=1,\n",
    "        num_attention_heads=64,\n",
    "        intermediate_size=16384,\n",
    "        inner_group_num=1,\n",
    "        hidden_act=\"gelu_new\",\n",
    "        hidden_dropout_prob=0,\n",
    "        attention_probs_dropout_prob=0,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        classifier_dropout_prob=0.1,\n",
    "        pad_token_id=0,\n",
    "        bos_token_id=2,\n",
    "        eos_token_id=3,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_hidden_groups = num_hidden_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.inner_group_num = inner_group_num\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.classifier_dropout_prob = classifier_dropout_prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$ GeluNew = 0.5  x  (1 + (x + 0.044715 * x^3)tanh(\\sqrt{2 / pi}  )) $\n",
    "\n",
    "$ swish = x * sigmoid(x) $\n",
    "\n",
    "$ mish = x * tanh(softplus(x)) $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "AlbertTransformer(\n",
      "  (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n",
      "  (albert_layer_groups): ModuleList(\n",
      "    (0): AlbertLayerGroup(\n",
      "      (albert_layers): ModuleList(\n",
      "        (0): AlbertLayer(\n",
      "          (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): AlbertAttention(\n",
      "            (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "<generator object Module.parameters at 0x00000205FFAA6448>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def mish(x):\n",
    "    return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def gelu_new(x):\n",
    "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "gelu = F.gelu\n",
    "gelu_new = torch.jit.script(gelu_new)\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish, \"gelu_new\": gelu_new, \"mish\": mish}\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )        \n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)  # 排列这个张量的维数。\n",
    "    \n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        if encoder_hidden_states is not None:\n",
    "            mixed_key_layer = self.key(encoder_hidden_states)\n",
    "            mixed_value_layer = self.value(encoder_hidden_states)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "            \n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "            \n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "            \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "class AlbertAttention(BertSelfAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pruned_heads = set()\n",
    "        \n",
    "    def prune_heads(self, heads):\n",
    "        ... \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, head_mask=None):   \n",
    "        mixed_query_layer = self.query(input_ids)\n",
    "        mixed_key_layer = self.key(input_ids)\n",
    "        mixed_value_layer = self.value(input_ids)\n",
    "        \n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # 用“查询”和“关键字”之间的点积来得到原始的注意力分数。\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            # 应用注意遮罩(在BertModel forward()函数中为所有层预先计算)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "        \n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        \n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # Should find a better way to do this\n",
    "        w = (self.dense.weight.t()\n",
    "                .view(self.num_attention_heads, self.attention_head_size, self.hidden_size)\n",
    "                .to(context_layer.dtype)\n",
    "        )\n",
    "        b = self.dense.bias.to(context_layer.dtype)\n",
    "\n",
    "        projected_context_layer = torch.einsum(\"bfnd,ndh->bfh\", context_layer, w) + b\n",
    "        projected_context_layer_dropout = self.dropout(projected_context_layer)\n",
    "        layernormed_context_layer = self.LayerNorm(input_ids + projected_context_layer_dropout)\n",
    "        return (layernormed_context_layer, attention_probs) if self.output_attentions else (layernormed_context_layer,)\n",
    "        \n",
    "\n",
    "class AlbertLayer(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.attention = AlbertAttention(config)\n",
    "        self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.activation = ACT2FN[config.hidden_act]\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask = None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        ffn_output = self.ffn(attention_output[0])\n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.ffn_output(ffn_output)\n",
    "        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n",
    "        \n",
    "        return (hidden_states, ) + attention_output[1:]\n",
    "        \n",
    "\n",
    "class AlbertLayerGroup(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.albert_layers = nn.ModuleList(AlbertLayer(config) for _ in range(config.inner_group_num))\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask= None):\n",
    "        layer_hidden_states = ()\n",
    "        layer_attentions = ()\n",
    "        \n",
    "        for layer_index, albert_layer in enumerate(self.albert_layers):\n",
    "            layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index])\n",
    "            hidden_states = layer_output[0]\n",
    "            \n",
    "            if self.output_attentions:\n",
    "                layer_attentions += (layer_output[1], )\n",
    "                \n",
    "            if self.output_hidden_states:\n",
    "                layer_hidden_states += (hidden_states, )\n",
    "                \n",
    "        outputs = (hidden_states, )\n",
    "        if self.output_hidden_states:\n",
    "            outputs += (layer_hidden_states, )\n",
    "        if self.output_attentions:\n",
    "            outputs += (layer_attentions, )\n",
    "        return outputs\n",
    "            \n",
    "\n",
    "\n",
    "class AlbertTransformer(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.embedding_hidden_mapping_in = nn.Linear(config.embedding_size, config.hidden_size)\n",
    "        self.albert_layer_groups = nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n",
    "        \n",
    "        all_attentions = ()\n",
    "        \n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = (hidden_states)\n",
    "            \n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            \n",
    "            layers_per_group = self.config.num_hidden_layers // self.config.num_hidden_groups\n",
    "            \n",
    "            group_idx = i // (self.config.num_hidden_layers / self.config.num_hidden_groups)\n",
    "            \n",
    "            layer_group_output = self.albert_layer_groups[group_idx](\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group]\n",
    "            )\n",
    "            hidden_states = layer_group_output[0]\n",
    "            \n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + layer_group_output[-1]\n",
    "                \n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "                \n",
    "        outputs = (hidden_states,)\n",
    "        \n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states, )\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions, )\n",
    "            \n",
    "        return outputs\n",
    "            \n",
    "albert_xxlarge_configuration = AlbertConfig()            \n",
    "albertTransformer = AlbertTransformer(albert_xxlarge_configuration)\n",
    "\n",
    "print(albertTransformer)\n",
    "print(albertTransformer.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "AlbertTransformer(\n",
      "  (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n",
      "  (albert_layer_groups): ModuleList(\n",
      "    (0): AlbertLayerGroup(\n",
      "      (albert_layers): ModuleList(\n",
      "        (0): AlbertLayer(\n",
      "          (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "          (attention): AlbertAttention(\n",
      "            (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "<generator object Module.parameters at 0x00000205FF85E4C8>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "albert_base_configuration = AlbertConfig(\n",
    "                hidden_size=768,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "            )\n",
    "albertTransformer = AlbertTransformer(albert_xxlarge_configuration)\n",
    "\n",
    "print(albertTransformer)\n",
    "print(albertTransformer.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
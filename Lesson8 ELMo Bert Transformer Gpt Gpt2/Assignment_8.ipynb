{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 自编码是一种神经网络模型，属于无监督学习，主要用于降维和特征提取。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: Beam search 也属于一种贪心算法。当Beam Size 设置为1时，等同于 Greedy Search。\n",
    "\n",
    "参考：https://blog.csdn.net/qq_16234613/article/details/83012046"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 注意力机制是模仿人类的注意力的一种对数据的处理算法，也是一种思想。人类注意力就是侧重于某些信息而\n",
    "忽略其他信息来从海量信息中筛选提取少数高质量信息的方式。相似的，假设要生成某个结果，决定这个结果的N个\n",
    "因素，对这个结果的影响如果都是相同的话，我们认为这是没有加入注意力概念的；相反如果让这些因素能够有一些\n",
    "侧重点，这些因素对结果影响较大，而让其他因素影响较小。这样就像是人类注意力一样，这也就是注意力机制。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 1. 不能解决一词多义问题\n",
    "     2. 不能区分和解决同义词反义词等深层次语义问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: ELMo是以LSTM为基本单元，构成双向的两层或多层且多通道的神经网络模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: Transformer 抛弃了RNN的循环结构，引入了注意力机制和位置Embedding，使Transformer同样具有记忆能力，\n",
    "相比RNN的记忆能力，Transformer的记忆全局性更好。 另外它抛弃了循环结构，使得它训练不依赖于上一个状态的结果，\n",
    "所以能提高并行计算能力。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 因为 batch normalization是对batch做归一化，会受到 batch size 的影响，特别是batch size较小的时候，\n",
    "影响很大。而 Layer Normalization 则不存在这样的问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 因为 Transformer 的输入是把句子中的所有词同时输入，如果直接输入到神经网络的话，神经网络的输出并不能很好地体现出每个输入的位置。\n",
    "所以引入Position Embedding 来解决位置对结果的影响。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: self-attention 自注意力机制，简单说就是：当网络输入一句话中的多个词的词向量后，先乘以Wq，Wk，Wv （q = quary, k = key, v = value）\n",
    "得到各个词的Qi,Ki,Vi, 然后每个词向量都进行一次 Vi * softmax((Qi x Kj.T / qirt(词向量维度))), 其中 Kj是表示各个词的K。 最后得到的计算结果\n",
    "就是加入自注意力后的结果。\n",
    "nulti-head attention 多头注意力机制，它其实是计算了多个self-attention后拼接在一起来得到。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: GPT 和 Bert 的基本单元都是 Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 如果已经训练好了一个NLP任务的GPT模型，那么使用迁移学习技巧，可不需要重新训练整个模型，而是把已经训练好的模型的一部分或全部的输出\n",
    "接在新任务的神经网络上，集中训练新任务的神经网络模型，这样可以节省大量的训练时间。\n",
    "具体哪类NLP任务，结合特定任务的特点，去构造末端的神经网络。\n",
    "\n",
    "现成的 GPT-2 的使用可以参考：\n",
    "https://www.jianshu.com/p/facbf5fca3da "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: Masked Multi-head Attention， 拿周围的词预测中间挖掉的词的注意力模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: BEAT的输入是由词向量、句向量、位置向量组合而成。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: 同GPT一样也是迁移学习，在末端根据不同的任务去构造神经网络模型。 \n",
    "若是单句子分类任务，如情感分析任务，使用[CLS]标签对应的输出，接一个Linear层，然后softmax即可；\n",
    "若是多标签任务，那么用每个词对应的输出相应的做一个分类即可；\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ans: GPT 是多层单向的Transformer模型。\n",
    "GPT-2 结构上和GPT是一样的，只是单元数量更多。\n",
    "BERT 是多层双向的Transformer模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}